# Chapter6 Hive

> 王嘉鹏 shenhao

## 6.1 数据仓库

> ps：到了数据仓库啦，崭新的饱满的和知识相濡以沫的一天，又开始啦！！！！

![](https://gitee.com/shenhao-stu/Big-Data/raw/master/doc_imgs/ch6.1.0.png)

### 6.1.1 为什么要有数据仓库

![](https://gitee.com/shenhao-stu/Big-Data/raw/master/doc_imgs/ch6.1.1.png)
数据的作用有两个：操作型记录的保存和分析型决策的制定。

1. 操作型记录的保存通常不必维护历史数据，只需修改数据以反映最新的状态；
2. 而分析型决策需要保存历史的数据，从而可以更精确的来评估现有状况进行决策。

基于后者分析型决策的优化，需要高性能的完成用户的查询，因此引出了数据仓库的概念。

### 6.1.2 数据仓库的主要特征
![](https://gitee.com/shenhao-stu/Big-Data/raw/master/doc_imgs/ch6.1.2-1.png)

数据仓库的主要特征是：主题性、集成性、非易失性、时变性的数据集合，用以支持管理决策。

1. 主题性

   各个业务系统的数据可能是相互分离的，但数据仓库则是**面向主题的**。数据仓库将不同的业务进行归类并进行分析，抽象为主题，对应于某一分析领域所涉及的分析对象。
   而操作型处理（传统数据）对数据的划分并不适用于决策分析。在数据仓库中，基于主题组织的数据被划分为各自独立的领域，每个领域有各自的逻辑内涵但互不交叉，在抽象层次上对数据进行完整、一致和准确的描述。

   拿零售业务的过程来举例：

   ![](https://gitee.com/shenhao-stu/Big-Data/raw/master/doc_imgs/ch6.1.2-2.png)

2.  集成性

   确定主题之后，就需要获取和主题相关的数据。这些数据会分布在不同的业务过程中，因此在数据进入数仓之前，需要对这些数据的口径进行统一。

   口径统一指的是：统一数据来源中的歧义、单位、字长等元素，进行总和计算，来聚合成新的数据。

   拿零售业务过程中的订单主题来举例，对于订单主题，通常会包括三个业务过程：订单、发货和发票。这些过程会产生一些新的指标，如：销售额、发票收入等。

   ![](https://gitee.com/shenhao-stu/Big-Data/raw/master/doc_imgs/ch6.1.2-3.png)

3. 非易失性

   数据仓库的目的是去分析数据中的规律，因此进入到数仓中的数据，需要保证其稳定，不会轻易丢失和改变。
   这里和传统操作型数据库的区别在于：操作型数据库主要服务于日常的业务操作，使得数据库需要不断地对数据实时更新，以便迅速获得当前最新数据，不至于影响正常的业务运作。而数据仓库通常是保存过渠的业务数据，根据业务需要每隔一段时间将一批新的数据导入数据仓库。

4. 时变性

   数仓是根据业务需要来建立的，其是一个项目，更代表了一个业务过程。因此数据仓库分析的结果只能反映过去的业务情况，当业务变化后，数据仓库需要跟随业务的变化而改变，以适应决策的需要。

## 6.2 数仓建模
![](https://gitee.com/shenhao-stu/Big-Data/raw/master/doc_imgs/ch6.2.png)
所有的数仓建模都围绕着两个目的：

1. 提供高效的查询性能；
2. 确保用户更方便的去理解数据。（业务过程）

基于此，数仓建模主要有两种方法：**维度建模**和ER模型。

### 6.2.1 维度和ER模型对比

1. 维度建模从**分析决策的需求**出发构建模型，为分析需求服务。重点关注用户如何更快速的完成需求分析，同时支持复杂查询。

2. ER模型建设数仓模型的出发点是**整合数据**，将各个系统中的数据按主题进行相似性组合和合并，并进行一致性处理，为数据分析决策服务，但不能直接用于分析决策。

下面我们以大学教育需求业务为例子，来简单了解下维度建模的数仓设计过程。

### 6.2.2 例子-大学教育数仓

假设大学现在有一个这样的需求：大学期望了解并理解学生在学校经历的各个方面，希望按照每学期课程和相关研究结果，来分析学生们对什么感兴趣以及工作职业的可能方向。

在维度模型设计时，通常的过程方法是四个步骤：

1. **选择业务过程**：业务过程通常用来表示业务执行的活动，如：学生选课、购买商品等；
2. **声明粒度**：粒度在数仓中是用来叙述表每行所想表达的含义，通常根据粒度来确定需要建立哪些事实表。通过事实表来跟踪记录事件的发生。事实表通常表示为“谁、在什么时间、做了什么事”。
3. **确定维度**：确定维度即是将事实表进行拆分，将拆分的数据作为维度存放。
4. **确定事实**：按照拆分的维度对数据进行重新聚合生成新的明细表。

根据学生的行为情况，对于大学教育业务可以分为以下四个核心过程：学生选课、课程注册、设施使用和学生考勤。接下来分别对这四个业务做简要说明。

![](https://gitee.com/shenhao-stu/Big-Data/raw/master/doc_imgs/ch6.2.2-1.png)

① 学生选课

学生选课的过程可以看作是一个漏斗，从最初的查询课程、提交申请、完成选课、选课更改。希望按照每个阶段的不同特点来分析学生的情况。

![](https://gitee.com/shenhao-stu/Big-Data/raw/master/doc_imgs/ch6.2.2-2.png)

② 课程注册

用事实表来跟踪学生课程注册的情况，事实表粒度是按每个学生每学期的注册课程为一行。

![](https://gitee.com/shenhao-stu/Big-Data/raw/master/doc_imgs/ch6.2.2-3.png)

③ 设备使用

大学回对设备进行投资，因此也需要了解每个学期中每天每个小时哪个设备被用于什么目的，统计的字段可以是：哪个设备使用率最高？设备的平均使用率是多少？周五时由于多数人不想来上课是否会导致设备使用率下降？

![](https://gitee.com/shenhao-stu/Big-Data/raw/master/doc_imgs/ch6.2.2-4.png)

④ 学生考勤

针对于学生考勤的需求，需要去了解学期中哪门课程的出勤率最低？哪个学生参加哪门课程？以及哪个老师的课最受学生欢迎等等需求。考勤的粒度可以是每天上课的学生为一行。

![](https://gitee.com/shenhao-stu/Big-Data/raw/master/doc_imgs/ch6.2.2-5.png)

通过这样几个场景的案例，我们应该大致可以了解数仓的作用，以及建模的基本流程。

## 6.3 Hive基本概念
![](https://gitee.com/shenhao-stu/Big-Data/raw/master/doc_imgs/ch6.3.png)
### 6.3.1 概述

**Hive是建立在Hadoop之上的一种数仓工具**。工具的用处是将结构化、半结构化的数据文件映射为一张数据库表，基于表提供了一种类似SQL的查询模型（HQL），用于访问和分析存储在Hadoop文件中的大型数据集。

Hive本身并不具备存储功能，其核心是将HQL转换为MapReduce程序，然后将程序提交到Hadoop集群执行。

![](https://gitee.com/shenhao-stu/Big-Data/raw/master/doc_imgs/ch6.3.1_1.png)

### 6.3.2 产生背景

![](https://gitee.com/shenhao-stu/Big-Data/raw/master/doc_imgs/ch6.3.1_2.png)

Hive的产生背景主要有两块：

其一是MR直接处理数据时，需要掌握java，学习成本比较高，而且使用MapReduce不容易实现复杂查询；

其二是建立分析型数仓的需求，Hive支持类SQL的查询以及支持自定义函数，可以作为数据仓库的软件。

hive利用hdfs存储数据，利用mapreduce查询分析数据。将sql转换为mapreduce程序从而完成对数据的分析。



> ps：看完了Hive产生的背景，一定很好奇这东西的内部构造是啥吧，客官请跟我来
>
> 坚持学习，沉迷学习，忘我学习，冲冲冲！！！

<img src="https://gitee.com/shenhao-stu/Big-Data/raw/master/doc_imgs/ch6.3.2_emoji.png" style="zoom:80%;" />



### 6.3.3 模拟实现Hive

这里通过一个简单的需求，来帮助我们更好的理解Hive原理。

**需求**：在HDFS文件系统上有一个文件，其内容如下：

![](https://gitee.com/shenhao-stu/Big-Data/raw/master/doc_imgs/ch6.3.3_1.png)

需要来设计hive，要求是通过这个hive，能够实现用户编写sql语句，来处理位于HDFS上结构化的数据，从而统计**来自北京**的**年龄大于20**的人数。

**分析**：写sql的前提是，对表进行操作，而不能是针对文件。那么需要记录文件和表之间的对应关系。

![](https://gitee.com/shenhao-stu/Big-Data/raw/master/doc_imgs/ch6.3.3_2.png)

要实现如上所示文件和表的对应关系，关键在于**实现表和文件的映射**，那么需要记录的信息包括：

1. 表是对应于哪个文件的；（表的位置信息）
2. 表的列是对应文件的哪一个字段；
3. 文件字段之间的分隔符是什么；

完成了表和文件的映射后，hive需要对用户写的sql进行语法校验，并且根据记录的元数据信息对sql进行解析，制定执行计划，并将执行计划转化为MapReduce程序来执行，最终将执行的结果封装返回给用户。

接下来在核心概念中我们来进一步了解下表和文件的映射信息。

## 6.4 核心概念

### 6.4.1 元数据

① **metadata**

上面所介绍的hive表和文件的映射信息，就叫做元数据。元数据是用来指**描述数据的数据**，对于Hive来说，元数据就是用来描述HDFS文件和表的各种对应关系（位置关系、顺序关系、分隔符）。Hive的元数据存储在**关系数据库**中（Hive内置的是Derby、第三方的是MySQL），HDFS存储的是数据。

② **metastore**

metastore是**元数据服务**：指的是hive操作管理访问元数据的一个服务。其作用是用来访问元数据，metastore对外暴漏一个服务地址，让客户端连接，保证hive元数据的安全。

使用metastore的好处在于：

1. 元数据把数据保存在关系数据库中，Hive做了元数据服务，能够访问操作元数据的，只有自己这个服务，对外暴漏一个服务地址，让客户端连接；
2. 支持多个客户端的连接，而客户端无需关心数据存在哪里，实现了解耦操作。

![](https://gitee.com/shenhao-stu/Big-Data/raw/master/doc_imgs/ch6.4.1_1.png)



元数据对Hive非常重要，metastore管理元数据的方式有三种：**内嵌模式**、**本地模式** 和 **远程模式**。

1. **内嵌模式**
   metastore**默认的**部署模式，是metastore元数据服务和hive服务融合在一起。

   ![](https://gitee.com/shenhao-stu/Big-Data/raw/master/doc_imgs/ch6.4.1_2.png)

   在这种模式下，Hive服务（Hive驱动本身）、元数据服务Metastore，元数据metadata（存储映射信息的）都在同一个JVM进程中，元数据存储在内置的**Derby数据库**。当启动HiveServer进程时，Derby和metastore都会启动，不需要额外的启Metastore服务。
   • **缺点**：一次只能支持一个活动用户，适合测试使用

2. **本地模式**
   本地模式与内嵌模式的区别在于：把元数据提取出来，让Metastore服务与主HiveServer进程在同一JVM进程中运行，存储元数据的数据库在单独的进程中运行。元数据一般存储在MySQL关系型数据库中。

   ![](https://gitee.com/shenhao-stu/Big-Data/raw/master/doc_imgs/ch6.4.1_3.png)

   • **缺点**：每启动一个hive服务，都会启动一个metastore服务。多个人使用时，会启用多个metastore服务没有统一。

3. **远程模式**
   既然可以把元数据存储给提取出来，考虑把metastore给提取出来变为单独一个。把metastore单独的配置，单独的对它进行启动，让它自己单独的在一个JVM上运行，保证全局唯一，从而保证数据访问的安全性。（不随hive的启动而启动）

   ![](https://gitee.com/shenhao-stu/Big-Data/raw/master/doc_imgs/ch6.4.1_4.png)

   • **优点**：把MetaStore服务独立出来，安装到远程的服务器集群里，从而解耦Hive服务和MetaStore服务，保证Hive运行的稳定性

### 6.4.2 Hive数据模型
Hive的数据都是存储在HDFS上的，默认有一个根目录，在hive-site.xml中。Hive数据模型的含义是用来描述：Hive是怎么来组织、管理和操作数据的。

Hive包含四种数据模型：**库，表，分区，分桶**。

1. **库**

​	mysql中默认数据库是default，用户可以创建不同的database，在database下也可以创建不同的表。 hive也可以分为不同的数据（仓）库，和传统数据库保持一致。在传统数仓中创建database。默认的数据库也是default。	Hive中的库相当于关系数据库中的命名空间，它的作用是将用户和数据库的表进行隔离。

2. **表**
   Hive中的表所对应的数据是存储在HDFS中，而表相关的元数据是存储在关系数据库中。

   Hive中的表有两种类型：内部表和外部表。

   两者的区别在于数据的访问和删除：

   1. 内部表的加载数据和创建表的过程是分开的，在加载数据时，实际数据会被移动到数仓目录中，之后对数据的访问是在数仓目录实现。而外部表加载数据和创建表是一个过程，对数据的访问是去读hdfs的数据；
   2. 内部表删除时，因为数据移动到了数仓目录中，故删除表时，表中数据和元数据会被同时删除。外部表因为数据还在HDFS中，删除表时不影响数据。

3. **分区**
   分区是一个优化的手段，目的是**减少全表扫描**，提高查询效率。在Hive存储的体现就是表的主目录文件夹下的子文件夹，子文件夹的名字就是定义的分区列的名字。

4. **分桶**
   分桶和分区的区别在于：分桶是针对数据文件本身来拆分的，根据表中字段（例如“编号ID”）的值,经过hash计算规则将数据文件划分成指定的若干个小文件。分桶后，数据底层会变为多个小文件。分桶的好处是可以**优化join查询和方便抽样查询**。

## 6.5 本章小结

hive不是数据库，而是面向数据分析的数据仓库。

> ps：多用脑，多思考，这一章内容很干，希望大家足够肝。
>
> 保护眼睛，保护头发，好好学习，天天向上

<img src="https://gitee.com/shenhao-stu/Big-Data/raw/master/doc_imgs/ch6.5.png" style="zoom:80%;" />
# Chapter10 期中大作业

> 边圣陶，蒋志政，王洲烽

> &emsp;&emsp;本章节对前三章的内容掌握程度进行一个小小的测试，包括一些面向企业的面试题，和基础的实战编程题。也算作最后的收尾工作，以实战作为我们课程的结束。

## 10. 1 面试题

### 10.1.1 hive外部表和内部表的区别

内部表的数据由hive管理，且存储在hive.metastore.warehouse.dir配置下的路径中；外部表的数据由HDFS存储，路径可以自己指定。

删除表时，内部表会把元数据及真实数据删除；外部表不删除真实数据。

### 10.1.2 简述对Hive桶的理解？

桶是对数据某个字段进行哈希取值，然后放到不同文件中存储。 数据加载到桶表时，会对字段取hash值，然后与桶的数量取模。把数据放到对应的文件中。物理上，每个桶就是表(或分区）目录里的一个文件，一个作业产生的桶(输出文件)和reduce任务个数相同。

### 10.1.3 HBase和Hive的区别？

|          |                       HBase                        |       Hive       |
| :------: | :------------------------------------------------: | :--------------: |
|   类型   |                     列式数据库                     |     数据仓库     |
| 内部机制 |                     数据库引擎                     |    MapReduce     |
| 增删改查 |                       都支持                       | 只支持导入和查询 |
|  Schema  | 只需要预先定义列族，不需要具体到列，列可以动态修改 | 需要预先定义表格 |
| 应用场景 |                        实时                        |     离线处理     |
|   特点   |                   以K-V形式存储                    |      类SQL       |

Hive和HBase是两种基于Hadoop的不同技术——Hive是一种类SQL的引擎，并且运行MapReduce任务，HBase是一种在Hadoop之上的NoSQL 的Key/vale数据库。当然，这两种工具是可以同时使用的。就像用Google来搜索，用Facebook进行社交一样，Hive可以用来进行统计查询，HBase可以用来进行实时查询，数据也可以从Hive写到HBase，设置再从HBase写回Hive

### 10.1.4 简述Spark宽窄依赖

窄依赖是指父RDD的每个分区只被子RDD的一个分区所使用，**子RDD分区通常对应常数个父RDD分区**(O(1)，与数据规模无关)

相应的，宽依赖是指父RDD的每个分区都可能被多个子RDD分区所使用，**子RDD分区通常对应所有的父RDD分区**(O(n)，与数据规模有关)

<img src="https://uploadfiles.nowcoder.com/files/20221019/234314825_1666189198264/1666187439425-907574ff-77f9-4edd-a991-a531695173ae.png" alt="img" style="zoom:80%;" />

### 10.1.5 Hadoop和Spark的相同点和不同点

**相同点：**

1）Hadoop和Spark都是并行计算，两者都是用MR模型进行计算。

2）都提供了灾难恢复

- Hadoop将每次处理后的数据写入磁盘中，对应对系统错误具有天生优势。
- Spark的数据对象存储在弹性分布式数据集(RDD)中。这些数据对象既可放在内存，也可以放在磁盘，所以RDD也提供完整的灾难恢复功能。

**不同点：**

1）Hadoop将中间结果存放在HDFS中，每次MR都需要刷写-调用，而Spark中间结果存放优先存放在内存中，内存不够再存放在磁盘中，不放入HDFS，避免了大量的IO和刷写读取操作；

2）Hadoop底层使用MapReduce计算架构，只有map和reduce两种操作，表达能力比较欠缺，而且在MR过程中会重复的读写HDFS，造成大量的磁盘IO读写操作，所以适合高时延环境下批处理计算的应用；Spark是基于内存的分布式计算架构，提供更加丰富的数据集操作类型，主要分成转化操作和行动操作，包括map、reduce、filter、flatmap、groupbykey、reducebykey、union和join等，数据分析更加快速，所以适合低时延环境下计算的应用；

3）Spark与Hadoop最大的区别在于迭代式计算模型。基于MapReduce框架的Hadoop主要分为map和reduce两个阶段，所以在一个job里面能做的处理很有限，对于复杂的计算，需要使用多次MR；Spark计算模型是基于内存的迭代式计算模型，根据用户编写的RDD算子和程序，在调度时根据宽窄依赖可以生成多个Stage，根据action算子生成多个Job。所以Spark相较于MapReduce，计算模型更加灵活，可以提供更强大的功能。

4）由于Spark基于内存进行计算，在面对大量数据且没有进行调优的情况下，可能会出现比如OOM内存溢出等情况，导致spark程序可能无法运行起来，而MapReduce虽然运行缓慢，但是至少可以慢慢运行完。

5）Hadoop适合处理静态数据，对于迭代式流式数据的处理能力差；Spark通过在内存中缓存处理的数据，提高了处理流式数据和迭代式数据的性能。

### 10.1.6 Spark为什么比MapReduce块？

1）基于内存计算，减少低效的磁盘交互；

2）高效的调度算法，基于DAG；

3）容错机制Linage，精华部分就是DAG和Lingae

### 10.1.7 说说你对Hadoop生态的认识

Hadoop生态主要分为三大类型，并以此展开来说（开放式）

1）分布式系统：HDFS，HBase

2）分布式计算引擎：Spark，MapReduce

3）周边工具：如zookeeper，pig，hive，oozie，sqoop，ranger，kafka等

## 10.2 实战

从新闻文章中发现热门话题和趋势话题是舆论监督的一项重要任务。在这个项目中，你的任务是使用新闻数据集进行文本数据分析，使用 Python 中 pySpark 的 RDD 和 DataFrame 的 API。问题是计算新闻文章数据集中每年各词的权重，然后选择每年 top-**k**个最重要的词。

### 10.2.1 数据集

您将使用的数据集包含多年来发布的新闻标题数据。在这个文本文件中，每一行都是一篇新闻文章的标题，格式为“ date,term1 term2... ...”。日期和文本用逗号分隔，文本用空格字符分隔。示例文件如下:

```
20030219,council chief executive fails to secure position
20030219,council welcomes ambulance levy decision
20030219,council welcomes insurance breakthrough
20030219,fed opp to re introduce national insurance
20040501,cowboys survive eels comeback
20040501,cowboys withstand eels fightback
20040502,castro vows cuban socialism to survive bush
20200401,coronanomics things learnt about how coronavirus economy
20200401,coronavirus at home test kits selling in the chinese community
20200401,coronavirus campbell remess streams bear making classes
20201015,coronavirus pacific economy foriegn aid china
20201016,china builds pig apartment blocks to guard against swine flu
```

### 10.2.2 文本权重计算

您需要忽略诸如“ to”、“ the”和“ in”之类的停用词。该文件存储了停用词。

为了计算一个文本的为期一年的权重，请使用 TF/IDF 模型。具体而言，TF 和 IDF 可计算为:

$TF(文本 t, 年份 y) = 在 y 年份中，包含文本 t 的新闻标题数量$

$IDF(文本 t,数据集 D) = log10 (在数据集D中年份的总数/含有文本t的年份总数) $

最后，文本 t 对于年份 y 的权重计算如下:

$Weight(文本 t, 年份 y, 数据集 D) = TF(文本 t, 年份 y)* IDF(文本 t, 数据集 D) $

请使用 `import math` 并使用 `math.log10()`计算文本权重，并将结果四舍五入到小数点后6位。

### 10.2.3 输出格式

如果数据集中有 N 年，那么您应该在最终输出文件中输出正好 N 行，并且这些行按年份升序排序。在每一行中，您需要以`<term, weight> `的格式输出 k 对list，并且这些对按照文本权重降序排序。如果两个文本具有相同的权重，则按字母顺序对它们进行排序。具体来说，每行的格式类似于: 
“year**\t** Term1,Weight1;Term 2,Weight2;… …;Termk,Weightk” 。例如，给定上述数据集和 **k** = 3，输出应该是:

```
2003 council,1.431364;insurance,0.954243;welcomes,0.954243
2004 cowboys,0.954243;eels,0.954243;survive,0.954243
2020 coronavirus,1.908485;china,0.954243;economy,0.954243
```

### 10.2.4 运行指令

**Dataframe:**

```
spark-submit project_df.py file:///testcase_top_20/sample.txt file:///res_df file:///stopwords.txt k
```

**RDD:**

```
spark-submit project_rdd.py file:///testcase_top_20/sample.txt file:///res_rdd file:///stopwords.txt k
```

其中 `file://`后跟随本地文件目录地址。



> 看到这里的小伙伴们，我们的课程终于结束啦，给坚持这么久的自己点个赞鼓鼓掌，希望你能永远保持开源学习的动力，这也是我们的初衷。
